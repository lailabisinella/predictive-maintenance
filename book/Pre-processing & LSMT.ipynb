{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f85cc27-fcec-450b-94ce-5f1b7b51c4a9",
   "metadata": {},
   "source": [
    "# Pre-processing & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f3046-deaf-42d3-9b62-2b5d67ac7ee8",
   "metadata": {},
   "source": [
    "This notebook implements the pre-processing steps tailored for the LSTM model, building upon the techniques used for the Random Forest (RF) and Support Vector Machine (SVM) models. Therefore, it is recommended to review the *Pre-processing & RF & SVM* notebook beforehand to fully understand the steps outlined here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612e909-d73e-48f8-8f0d-e474acb37778",
   "metadata": {},
   "source": [
    "Firstly, the **`os`** library is used to interact with the operating system. By setting the logging level to `2`, it filters out **INFO** and **WARNING** messages, allowing only **ERROR** messages to be displayed. This adjustment improves readability by ensuring that only critical issues are visible, making error messages clearer and easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f98cbf5-1e49-482d-8acb-4f3edd55a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f998b-9a35-4faf-962b-ba9f1a7fe48e",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f61cb9-03b3-43a1-b5f4-10094f1b2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('./../data/dataset.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865a2594-983c-4841-a6b7-72ae874dce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679045, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset has been imported correctly\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840292f9-a990-4c16-8f97-3a1f93872d49",
   "metadata": {},
   "source": [
    "## Pre-processing steps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7897373-bea6-4a11-91d2-d1555b558a9c",
   "metadata": {},
   "source": [
    "The key distinction in the pre-processing between this notebook and the *Pre-processing & RF & SVM* one lies in the **reshaping of the data**, which is applied to both the **sliding window process** and **Random Under-Sampling (RUS)**.  \n",
    "\n",
    "For **Random Forest (RF)** and **Support Vector Machine (SVM)**, the data is structured as a **2D array**, where each row represents an independent instance with all features stacked. This format is suitable for traditional machine learning models, which do not account for temporal dependencies in their architecture. However, for **Long Short-Term Memory (LSTM)**, the data must be reshaped into a **3D array** to preserve the **sequential nature** of the time-series data. \n",
    "This transformation is crucial, as LSTM models rely on **temporal dependencies** rather than isolated feature vectors. Without maintaining this structure, the model would lose the ability to recognise patterns over time, limiting its effectiveness in capturing trends and predicting alerts accurately.\n",
    "Therefore, this is the reason why a separate notebook was created for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ba443-90be-47bc-b625-80d9c82cecfc",
   "metadata": {},
   "source": [
    "### Dataset optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0dcba-2a4c-4d68-8e4c-fd285dbccf36",
   "metadata": {},
   "source": [
    "As implemented in the *Pre-processing & RF & SVM* notebook, the `session_counter` and `time_to_failure` features are removed from the dataset, as they are not relevant to the classification task. Eliminating these features ensures that the model focuses solely on the meaningful variables that contribute to the prediction of `alert_11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c6c633-5476-4eb0-b9cd-ac7157b5e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['session_counter', 'time_to_failure'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dceb3738-38b9-4514-8191-335b37228aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flag roping</th>\n",
       "      <th>Platform Position [°]</th>\n",
       "      <th>Platform Motor frequency [HZ]</th>\n",
       "      <th>Temperature platform drive [°C]</th>\n",
       "      <th>Temperature slave drive [°C]</th>\n",
       "      <th>Temperature hoist drive [°C]</th>\n",
       "      <th>Tensione totale film [%]</th>\n",
       "      <th>Current speed cart [%]</th>\n",
       "      <th>Platform motor speed [%]</th>\n",
       "      <th>Lifting motor speed [RPM]</th>\n",
       "      <th>Platform rotation speed [RPM]</th>\n",
       "      <th>Slave rotation speed [M/MIN]</th>\n",
       "      <th>Lifting speed rotation [M/MIN]</th>\n",
       "      <th>alert_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-07 04:14:30.742</td>\n",
       "      <td>31.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-07 04:14:35.742</td>\n",
       "      <td>31.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-07 04:14:40.742</td>\n",
       "      <td>31.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-07 04:14:45.742</td>\n",
       "      <td>31.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-07 04:14:50.742</td>\n",
       "      <td>31.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp  Flag roping  Platform Position [°]  \\\n",
       "0 2021-06-07 04:14:30.742         31.0                  115.0   \n",
       "1 2021-06-07 04:14:35.742         31.0                  115.0   \n",
       "2 2021-06-07 04:14:40.742         31.0                  115.0   \n",
       "3 2021-06-07 04:14:45.742         31.0                  115.0   \n",
       "4 2021-06-07 04:14:50.742         31.0                  115.0   \n",
       "\n",
       "   Platform Motor frequency [HZ]  Temperature platform drive [°C]  \\\n",
       "0                         5200.0                             18.0   \n",
       "1                         5200.0                             18.0   \n",
       "2                         5200.0                             18.0   \n",
       "3                         5200.0                             18.0   \n",
       "4                         5200.0                             18.0   \n",
       "\n",
       "   Temperature slave drive [°C]  Temperature hoist drive [°C]  \\\n",
       "0                          22.0                          18.0   \n",
       "1                          22.0                          18.0   \n",
       "2                          22.0                          18.0   \n",
       "3                          22.0                          18.0   \n",
       "4                          22.0                          18.0   \n",
       "\n",
       "   Tensione totale film [%]  Current speed cart [%]  Platform motor speed [%]  \\\n",
       "0                     181.0                     0.0                     100.0   \n",
       "1                     181.0                     0.0                     100.0   \n",
       "2                     181.0                     0.0                     100.0   \n",
       "3                     181.0                     0.0                     100.0   \n",
       "4                     181.0                     0.0                     100.0   \n",
       "\n",
       "   Lifting motor speed [RPM]  Platform rotation speed [RPM]  \\\n",
       "0                        0.0                           84.0   \n",
       "1                        0.0                           84.0   \n",
       "2                        0.0                           84.0   \n",
       "3                        0.0                           84.0   \n",
       "4                        0.0                           84.0   \n",
       "\n",
       "   Slave rotation speed [M/MIN]  Lifting speed rotation [M/MIN]  alert_11  \n",
       "0                         116.0                             0.0       0.0  \n",
       "1                         116.0                             0.0       0.0  \n",
       "2                         116.0                             0.0       0.0  \n",
       "3                         116.0                             0.0       0.0  \n",
       "4                         116.0                             0.0       0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataset to ensure the columns have been removed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28d450-cf06-4843-9025-b1821bf3aeb3",
   "metadata": {},
   "source": [
    "As is common in most time-series datasets, the `Timestamp` column is set as the index of the dataset. This allows for efficient time-based operations, such as resampling, sliding window calculations, and trend analysis, while preserving the chronological structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90fd838b-b7ad-44f2-8592-f010fd511460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebcf54c-6886-4df6-afd3-7c2ae834ba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Timestamp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Timestamp has become the index of the dataset\n",
    "df.index.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a95863-f280-4728-9351-90575bcb30d5",
   "metadata": {},
   "source": [
    "### Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457aabd7-e454-4e3b-9479-661b966c4a26",
   "metadata": {},
   "source": [
    "In this section, the label `y` and the features `X` for the models are defined, with `alert_11` serving as the target variable (`y`) and all other columns being designated as features (`X`). This decision is based on the fact that `alert_11` represents the primary event of interest, aligning with the original study’s objective. By including all other columns as features, the model can leverage the full range of available data to identify patterns and relationships that may contribute to predicting `alert_11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb5d29a-a164-47a0-bd0b-38442602c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Label (shape=(1,)): ['alert_11']\n",
      "-> Features (shape=(13,)): ['Current speed cart [%]' 'Flag roping' 'Lifting motor speed [RPM]'\n",
      " 'Lifting speed rotation [M/MIN]' 'Platform Motor frequency [HZ]'\n",
      " 'Platform Position [°]' 'Platform motor speed [%]'\n",
      " 'Platform rotation speed [RPM]' 'Slave rotation speed [M/MIN]'\n",
      " 'Temperature hoist drive [°C]' 'Temperature platform drive [°C]'\n",
      " 'Temperature slave drive [°C]' 'Tensione totale film [%]']\n"
     ]
    }
   ],
   "source": [
    "# State the label and the features\n",
    "import numpy as np\n",
    "\n",
    "label = np.array(['alert_11'])\n",
    "features = np.array(df.columns.difference(label))\n",
    "\n",
    "print(f\"-> Label (shape={label.shape}): {label}\")\n",
    "print(f\"-> Features (shape={features.shape}): {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb802f8-188e-4b07-90c2-86bd5c266269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> X (shape=(679045, 13))\n",
      "-> y (shape=(679045, 1))\n"
     ]
    }
   ],
   "source": [
    "# Extract and assign the label and the features, X and y\n",
    "X = df[features]\n",
    "y = df[label]\n",
    "\n",
    "print(f\"-> X (shape={X.shape})\")\n",
    "print(f\"-> y (shape={y.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e9b90-ae1b-4113-8a52-0a46adbb81b5",
   "metadata": {},
   "source": [
    "### Sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9f425-0fbc-4749-a1f4-ccb29edb620e",
   "metadata": {},
   "source": [
    "Creating sliding windows as a pre-processing step is essential for capturing temporal dependencies in the dataset, allowing the model to analyse sequences of past sensor readings rather than isolated time points. \n",
    "Since machine failures and alerts often develop gradually, a single timestamp may not provide enough context for accurate predictions. However, by structuring the data into overlapping time windows, the model can learn meaningful trends and relationships that contribute to the occurrence of `alert_11`. \n",
    "This approach mimics real-world decision-making, where operators and automated systems consider historical data before identifying potential issues. \n",
    "Additionally, sliding windows help handle lag effects, ensuring that early warning signs (such as changes in temperature, speed, or vibrations) are accounted for. Furthermore, this method prevents data leakage by ensuring that predictions rely only on past information, making the model more robust for real-world deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63048f67-3154-42a6-8a41-11844ac215c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the label and features for the window\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5328aa7-cb0b-4a27-928b-c4534f7fc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the window\n",
    "import numpy as np  # Import NumPy for array manipulation\n",
    "\n",
    "x_wins_shape = None  # Placeholder to store the shape of the final X_wins \n",
    "\n",
    "# Function to create sliding windows of features and corresponding labels\n",
    "def window(X_data, y_data, width: int, shift: int):\n",
    "\n",
    "    X_wins, y_wins = [], []  # Initialise lists to hold windowed inputs and labels\n",
    "\n",
    "    # Iterate over all samples by index\n",
    "    for index, (X, y) in enumerate(zip(X_data, y_data)):\n",
    "\n",
    "        # Make sure the window (width + shift) does not exceed dataset bounds\n",
    "        if (index + width + shift) <= X_data.shape[0]:\n",
    "\n",
    "            # Define the target label slice (after the input window, for future prediction)\n",
    "            window = slice((index + width), (index + width + shift))\n",
    "\n",
    "            # Collect the feature window of specified width\n",
    "            X_wins.append(X_data[index: index + width])\n",
    "\n",
    "            # Get the corresponding labels in the future window\n",
    "            y_values_shift = y_data[window]\n",
    "\n",
    "            # Label the input window as 1 if any future label is 1 (breakdown happens)\n",
    "            y_wins.append(int(np.any(y_values_shift == 1)))\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    X_wins = np.array(X_wins)\n",
    "    x_wins_shape = X_wins.shape  # Save the shape for reference or debugging\n",
    "\n",
    "    y_wins = np.array(y_wins)\n",
    "\n",
    "    # Reshape input to 2D (flattened windows) and ensure y is 1D\n",
    "    return X_wins.reshape(X_wins.shape[0], -1), y_wins.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdd02420-2e5b-4173-8305-ac8b45333957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State the variables and the size of the window\n",
    "X_wins, y_wins = window(X, y, width=120, shift=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b61d9-ca2f-45d7-8358-2d5b6049198f",
   "metadata": {},
   "source": [
    "### Random Under Sampler (RUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693cecc-27e0-4ddc-9134-fb7c11e016a1",
   "metadata": {},
   "source": [
    "As stated in the paper:  \n",
    "\n",
    "> Alerts are anomalies and thus, by definition, rarer than normal behaviors.  \n",
    "\n",
    "This observation is supported by the exploratory data analysis (EDA) conducted in this study, which confirms that the dataset is highly imbalanced. Specifically, `alert_11` consists of **677,652 instances of 0s** and only **1,393 instances of 1s**, corresponding to **99.8%** and **0.2%** of the dataset, respectively.  \n",
    "\n",
    "This class imbalance issue was also acknowledged in the paper. Given that our study shares the same objective, **Random Under-Sampling (RUS)** is considered an appropriate method for balancing the dataset. As described in the paper:  \n",
    "\n",
    "> The algorithm (RUS) randomly selects and removes observations from the majority class until it achieves the desired equilibrium between the two classes. In the case of the wrapping machine, RUS is applied separately on each train set (comprising 4 folds) and test set (1 fold) for each combination of RW and PW sizes, to prevent the presence of similar data in the train and test sets (i.e., partially overlapping data).  \n",
    "\n",
    "By employing RUS to the **sliding window** data, we ensure that the models are trained on a more balanced dataset, reducing the bias toward the majority class (`alert_11 = 0`) and improving its ability to correctly predict rare alert occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b7180c-7496-4e13-84b0-a0089b01dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6648, 1, 1560)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X_res, y_res = rus.fit_resample(X_wins, y_wins)\n",
    "X_res = X_res.reshape(X_res.shape[0], 1, X_res.shape[1]) # Generates 1 empty array in the middle to have a 3D array\n",
    "print(X_res.shape)\n",
    "# print(np.unique(y_res, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c56812-7d80-47c8-9683-276abc8d0374",
   "metadata": {},
   "source": [
    "After applying Random Under-Sampling (RUS), it is not usually expected that the number of positive cases (1s) would exceed what was originally present in the raw dataset (1,393). However, in this case, the result is correct because the windowing process alters the distribution of labels. \n",
    "\n",
    "Initially, the dataset contained **677,652 instances of 0** and only **1,393 instances of 1** for the `alert_11` feature. However, when the **windowing function** is applied, each window is labelled as 1 if any future value within the shift period is 1. As a result, a single occurrence of 1 in the original data can lead to multiple windows being labelled as 1. Therefore, after windowing, the number of windows labelled as 1 becomes much larger than the original 1,393. \n",
    "\n",
    "**Random Under-Sampling** then balances the two classes by reducing the majority class (0) down to match the number of minority instances (1) found after windowing, without creating any new 1s. This is why there is a result of 3,324 instances for both 0 and 1 after resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9e231-f05c-4313-9667-cef4e0a8e059",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d37026-a281-4306-8bb7-d0e9ec28dfc0",
   "metadata": {},
   "source": [
    "To establish the credibility of this study, three models (two in the *Pre-processing & RF & SVM* notebook, another in this one) from the original research are implemented. However, it is important to emphasise that this study is not merely a replication of the dataset-associated paper. While the objectives and methodological approaches remain the same, the **techniques** and **models** **implementations slightly differ** from the original study.  \n",
    "\n",
    "One key distinction is that the code in this study is designed to be **simpler and more accessible**, making it easier to understand compared to the implementation in the paper’s GitHub repository ([GitHub Repository](https://github.com/nicolopinci/polimi_failure_prediction)). Despite its simplified approach, the effectiveness of this study remains uncompromised, as the results align closely with those presented in the original research.  \n",
    "\n",
    "By incorporating both **traditional machine learning models (RF, SVM)** and **a deep learning approach (LSTM)**, this study provides a comprehensive evaluation of different techniques in predictive maintenance, ensuring a well-rounded assessment of classification performance on the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd24841-2537-4e89-ad64-23dfcaa31933",
   "metadata": {},
   "source": [
    "Since this is a classification task, all models will be evaluated using **accuracy, precision, recall, and F1-score**, as these metrics provide a comprehensive assessment of a model's performance, particularly in detecting rare alert occurrences.  \n",
    "\n",
    "Additionally, **stratified and non-stratified 5-fold cross-validation** will be used for models' validation. This sightly differs from the original work as, in this work, there is a prefernce in the use of stratified 5-fold cross-validation rather than 5-fold cross-validation.\n",
    "\n",
    "> The validation procedure is also adapted to the characteristics of the different use cases. In the wrapping machine dataset, there are only 13 alarms, which yield ≈500 failure RWs in the whole time series. Thus, the number of failure RWs in the test set would be too small to test adequately the performances. Thus, we adopt a training and evaluation procedure based on k-fold cross-validation (with k = 5).\n",
    "\n",
    "The reason of this choice is that stratified 5-fold cross-validation ensures that each fold maintains the original class distribution, which is especially important for imbalanced datasets. In contrast, standard 5-fold cross-validation may create folds with uneven class ratios, leading to biased model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e479d8-ed62-451a-a11a-89be540dc7f6",
   "metadata": {},
   "source": [
    "### Long Short-Term Memeory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0c71a4-fe7f-4e58-887b-ce1fc97bd2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold-cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca9599a5-bdf9-457a-85df-6c472c1ce65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store metrics and predictions\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "fold_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de7f7e6f-5c24-4bde-8018-744f15a77e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 1s 3ms/step\n",
      "42/42 [==============================] - 1s 2ms/step\n",
      "42/42 [==============================] - 1s 3ms/step\n",
      "42/42 [==============================] - 1s 2ms/step\n",
      "42/42 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_res, y_res)):\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X_res[train_idx], X_res[val_idx]\n",
    "    y_train, y_val = y_res[train_idx], y_res[val_idx]\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=L2(0.001)), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        BatchNormalization(),\n",
    "        Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=L2(0.001))),\n",
    "        BatchNormalization(),\n",
    "        Bidirectional(LSTM(128, return_sequences=False, kernel_regularizer=L2(0.001))),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation=\"relu\", kernel_regularizer=L2(0.001)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation=\"relu\", kernel_regularizer=L2(0.001)),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy', Precision(), Recall()]\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predict on validation data\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "    # Append to full lists for final global evaluation\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Metrics for current fold\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    prec = precision_score(y_val, y_pred)\n",
    "    rec = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    fold_metrics.append({\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "331f26b7-397a-4def-aa02-76b85e911ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean metrics across all folds\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "mean_metrics = metrics_df.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867ded8c-8345-4aaf-b3e0-e1f19863f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Metrics across 5 folds:\n",
      "accuracy     0.740977\n",
      "precision    0.795472\n",
      "recall       0.650712\n",
      "f1_score     0.715275\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute mean metrics across all folds\n",
    "print(\"Mean of Metrics across 5 folds:\")\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f036b20-f8a8-4a86-9380-5602cd970d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (based on cross-validated predictions):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7041    0.8312    0.7624      3324\n",
      "           1     0.7941    0.6507    0.7153      3324\n",
      "\n",
      "    accuracy                         0.7410      6648\n",
      "   macro avg     0.7491    0.7410    0.7388      6648\n",
      "weighted avg     0.7491    0.7410    0.7388      6648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate final classification report\n",
    "print(\"Classification Report (based on cross-validated predictions):\")\n",
    "print(classification_report(all_y_true, all_y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictive-maintenance",
   "language": "python",
   "name": "predictive-maintenance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
